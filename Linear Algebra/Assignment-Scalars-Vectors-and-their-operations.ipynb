{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "edba9c10a8a2aa2e6783a4c7df914425",
     "grade": false,
     "grade_id": "cell-d6919b76a5243139",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Scalars, vectors and their operations\n",
    "<b><div style=\"text-align: right\">[TOTAL POINTS: 30]</div></b>\n",
    "\n",
    "In this assignment, you will learn how vectors can be used in the field of **Natural Language Processing**. Here you will implement the measure of `cosine similarity`, `dot product` and `L2 norm` to understand how they can be used to find similarity between texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "U904ppRDugWB",
    "nbgrader": {
     "checksum": "0e8a63f41438ab0f01c7902ded30d550",
     "grade": false,
     "grade_id": "cell-ed61f7cb3297c5c3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Import required libraries\n",
    "Run the cell below to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "bE1tAQ3ysCTK",
    "nbgrader": {
     "checksum": "94673e1496a5011482ef09c3393b787e",
     "grade": false,
     "grade_id": "cell-de5f7f1c8f915a09",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # numpy is used to perform mathematical operations\n",
    "import matplotlib.pyplot as plt # this library helps in plotting and visualizing results\n",
    "import pickle # this library is used to retrieve and store python objects from/to disks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "Iv0gyqNnumpZ",
    "nbgrader": {
     "checksum": "03318b6854a97151614c2ea70c032e6d",
     "grade": false,
     "grade_id": "cell-b5151da9d1c2a431",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Tokenize and vectorize sentences\n",
    "For this section you simply need to execute the cells below and observe how the code does the required operations. These cells vectorize sentences for comparision. \n",
    "Sentences or text in natural language consist of vocabulary which form the elemental part. The text must be vectorized in order to perform mathematical operations on them. Vectorization can be done in various ways. Here we will observe two ways:\n",
    "*  Sparse-sentence vectors\n",
    "*  Dense word vectors\n",
    "\n",
    "**Sparse sentence vectors**:\n",
    "In this method of vectorization vocabulary is first constructed by decomposing all sentences into tokens and keeping a dictionary of distinct tokens. The sentences are then vectorized, with each vector for a sentence representing the number of times a token occurs in a sentence. You will see how each of these is done in deatil below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "F0_LVkkrs_t6",
    "nbgrader": {
     "checksum": "5487d37c19fd6040ac9a7b4d60ad9fdf",
     "grade": false,
     "grade_id": "cell-485ff75d315c9997",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# First we define sentences that we want to compare.\n",
    "# Sentences are stored in a list\n",
    "\n",
    "sentences = [\"It is such a good day.\",\n",
    "             \"Hello!, how are you doing?\",\n",
    "             \"Such a wonderful day it is.\",\n",
    "             \"How do you do folks?\",\n",
    "             \"What time is it right now?\",\n",
    "             \"When is your birthday?\",\n",
    "             \"When do you usually return from work?\",\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "IukwQf3jzGVI",
    "nbgrader": {
     "checksum": "dd2db1a54347b52744585f505055a309",
     "grade": false,
     "grade_id": "cell-0aad9549cc73d28a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "'''the function below tokenizes a sentence\n",
    "i.e. it takes in a sentence as input and returns\n",
    "a list of words all in lowercase by removing any punctuations like full-stops(.), commas(,),\n",
    "question marks(?) and exclamation marks(!)'''\n",
    "\n",
    "def tokenize(sentence):\n",
    "    res  = [] # initialize empty list for resutls\n",
    "    for word in sentence.split(' '): # split the at spaces sentence to separate words \n",
    "        res.append(word.strip('?.,!').lower()) # strip the words and convert them to lowercase \n",
    "                                           # then remove the punctuations and append to list\n",
    "    return res # return tokens\n",
    "\n",
    "\n",
    "# using the tokenize() function prepare a list of tokenized sentences\n",
    "# from a list of sentences defined above\n",
    "# each element of tokenized_sentences list is another list of tokens\n",
    "# representing tokens generated from each individual sentence \n",
    "tokenized_sentences = [tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# create a list of distinct tokens\n",
    "tokens = [] # initilize list of tokens\n",
    "for sublist in tokenized_sentences: \n",
    "    for item in sublist:\n",
    "        tokens.append(item) # append each token from all sentences to the list of tokens\n",
    "\n",
    "# Since, the same token may be present in multiple sentences the list of tokens contains duplicates\n",
    "# remove the duplicates by converting the tokens into a set\n",
    "tokens = set(tokens) \n",
    "\n",
    "# now convert the set back to list which now does not contain any duplicate\n",
    "tokens = list(tokens)\n",
    "\n",
    "# then sort the list so that our vector can have a fixed sorted order\n",
    "tokens = sorted(tokens)\n",
    "\n",
    "# finally create a dictionary mapping each word to its index in the sorted list\n",
    "# this index for this token is then used to represent its position in a vector\n",
    "word2index = dict() # initialize a dictionary\n",
    "for i,token in enumerate(tokens):\n",
    "    word2index[token] = i # mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "M6qLdCh-MqyG",
    "nbgrader": {
     "checksum": "e2c39b683a71bb7c0134c6e89bf6c76c",
     "grade": false,
     "grade_id": "cell-34d6d567f64bc6c5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "6f2a961c-8312-451f-d975-1bf8f42dc2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0: 'It is such a good day.' \n",
      "sentence[0] is vectorized as follows:\n",
      "[1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "a  ->  1.0\n",
      "are  ->  0.0\n",
      "birthday  ->  0.0\n",
      "day  ->  1.0\n",
      "do  ->  0.0\n",
      "doing  ->  0.0\n",
      "folks  ->  0.0\n",
      "from  ->  0.0\n",
      "good  ->  1.0\n",
      "hello  ->  0.0\n",
      "how  ->  0.0\n",
      "is  ->  1.0\n",
      "it  ->  1.0\n",
      "now  ->  0.0\n",
      "return  ->  0.0\n",
      "right  ->  0.0\n",
      "such  ->  1.0\n",
      "time  ->  0.0\n",
      "usually  ->  0.0\n",
      "what  ->  0.0\n",
      "when  ->  0.0\n",
      "wonderful  ->  0.0\n",
      "work  ->  0.0\n",
      "you  ->  0.0\n",
      "your  ->  0.0\n"
     ]
    }
   ],
   "source": [
    "# now vectorize sentences by converting them into numpy arrays\n",
    "\n",
    "# first initilize a vector of zeros to accommodate\n",
    "# len(sentences) number of vectors\n",
    "# each with dimension len(tokens)\n",
    "vectors = np.zeros((len(sentences),len(tokens))) \n",
    "\n",
    "\n",
    "for i,sentence in enumerate(tokenized_sentences): # go through each sentence in the list\n",
    "    for j,word in enumerate(sentence): # go through each token in each sentence\n",
    "        vectors[i][word2index[word.lower()]] += 1   # update the number of times a token appears in the sentence\n",
    "    \n",
    "# see how the sentences have been vectorized by print them out\n",
    "print(\"Sentence 0: '{}' \".format(sentences[0]))\n",
    "print(\"sentence[0] is vectorized as follows:\")\n",
    "print(vectors[0])\n",
    "for i in range(len(tokens)):\n",
    "    print(tokens[i], \" -> \", vectors[0][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "Cv4d5YVNRaTt",
    "nbgrader": {
     "checksum": "4b368241ec2374c7e385b67361684953",
     "grade": false,
     "grade_id": "cell-06eee41f7e5e06fb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we have vectorized the sentences to create sparse vectors out of them. The vectors are called sparse because they are mostly as you have seen above. \n",
    "\n",
    "**Now it's your time to write some code**\n",
    "\n",
    "Below you will imeplement dot product of two vectors, the L2 norm of a vector and cosine similarity measure. These implementations will then be used to compare the similarity between all pairs of sentences in the list defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "IZdSDbCsuGjL",
    "nbgrader": {
     "checksum": "9f12a2836e51fdb3823833602081f4a5",
     "grade": false,
     "grade_id": "cell-4b6843d337c90ebd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "tags": [
     "Exercise-1"
    ]
   },
   "source": [
    "## Exercise 1: Implement dot product\n",
    "<b><div style=\"text-align: right\">[POINTS: 10]</div></b>\n",
    "Implement dot product for two vectors.\n",
    "\n",
    "**Task:** Build a function `dot_product(a,b)` that takes in two vectors `a` and `b` and returns the dot product of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "6bkusTbpuFAP",
    "nbgrader": {
     "checksum": "f7b7a4ad7e25b3cfe2fab90274253d32",
     "grade": false,
     "grade_id": "cell-6d8d9fdf7e05dfa0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "tags": [
     "Exercise-1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 12]\n"
     ]
    }
   ],
   "source": [
    "def dot_product(a,b):\n",
    "    return np.dot(a,b)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(dot_product([2,2],[[1,2],[3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "T80WCobEKQXn",
    "nbgrader": {
     "checksum": "8b8ef879c2036f3e26a2bd9a042ac301",
     "grade": true,
     "grade_id": "cell-765009665f6a71bb",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    },
    "tags": [
     "Exercise-1"
    ]
   },
   "outputs": [],
   "source": [
    "### INTENTIONALLY LEFT BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "7DW_MnmKuKcM",
    "nbgrader": {
     "checksum": "e2c34e0225d57c23b171652be84f472c",
     "grade": false,
     "grade_id": "cell-a7dbafec3c96ff7c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "tags": [
     "Exercise-2"
    ]
   },
   "source": [
    "## Exercise 2: Implement L2 norm\n",
    "\n",
    "<b><div style=\"text-align: right\">[POINTS: 10]</div></b>\n",
    "Implement L2 Norm for a vector.\n",
    "\n",
    "**Task:** Build a function `l2_norm(a)` that takes in one vector `a` and returns the L2 norm of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Z3fcX8iluFfW",
    "nbgrader": {
     "checksum": "f365dd4d92c83ba8ae8be290bc7356ff",
     "grade": false,
     "grade_id": "cell-ace57456a294be14",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "tags": [
     "Exercise-2"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "def l2_norm(a):\n",
    "    return np.sqrt(np.sum([i *i for i in a]))\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(l2_norm([3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "fMeX082YKHm-",
    "nbgrader": {
     "checksum": "2234a9af8eaa9fd3ea6f2eebb053c86d",
     "grade": true,
     "grade_id": "cell-d34d929851c751f8",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    },
    "tags": [
     "Exercise-2"
    ]
   },
   "outputs": [],
   "source": [
    "### INTENTIONALLY LEFT BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "ugY1Fo4tuPQr",
    "nbgrader": {
     "checksum": "44e6aff17238d95764552f03c2a73d41",
     "grade": false,
     "grade_id": "cell-e7a50a006b82e92c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "tags": [
     "Exercise-3"
    ]
   },
   "source": [
    "## Exercise 3: Calculate cosine similarity\n",
    "<b><div style=\"text-align: right\">[POINTS: 10]</div></b>\n",
    "Implement cosine similarity of two vectors. Cosine similarity is the cosine of the angle between two vectors. The lesser the angle between them, greater is their similarity. Since, `cos(0) = 1`, the vectors which are parallel (i.e. having all features in common) to each other receive a full score of `1` and orthogonal vectors (i.e. having no features in common) receive a score of `0`. \n",
    "\n",
    "**Task:** \n",
    "- Build a function `cosine_similarity(a,b)` that takes in two vectors `a` and `b` and returns the cosine of the angle between the vectors.\n",
    "- You can refer to the formula from the reading material to compute the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "E-81kXXpuRef",
    "nbgrader": {
     "checksum": "7147081cf42eab1e37c8f4e1240bbe93",
     "grade": false,
     "grade_id": "cell-43def4d771317390",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "tags": [
     "Exercise-3"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9761870601839528\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a,b):\n",
    "    numerator = np.sum([x*y for x,y in zip(a,b)])\n",
    "    denomenator = l2_norm(a) * l2_norm(b)\n",
    "    return numerator/denomenator\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(cosine_similarity([1,4],[2,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "iqBPhwVoo1KX",
    "nbgrader": {
     "checksum": "987dd785568c6ad773ae2e6506de53f8",
     "grade": true,
     "grade_id": "cell-cdad58cb20144c8d",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    },
    "tags": [
     "Exercise-3"
    ]
   },
   "outputs": [],
   "source": [
    "### INTENTIONALLY LEFT BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "fZrjJpKbsYNV",
    "nbgrader": {
     "checksum": "1330f5fc9e8642f7dc665000a088a9f4",
     "grade": false,
     "grade_id": "cell-797c4092667bdb1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Visualize cosine similarity between pairs:\n",
    "Now it is time to visualize how our effectiveness of **sparse vector model**  in measuring similarity, by using the code we have written together above . Execute the cell below and observe its working and the plot produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "z9SXB_lgsWpz",
    "nbgrader": {
     "checksum": "695a4e91dc395fb513cf2a0bc538bed4",
     "grade": false,
     "grade_id": "cell-43481c66b73e103e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "d4a1de05-42cb-40be-9418-6714ba7c9cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence  0 :  It is such a good day.\n",
      "Sentence  1 :  Hello!, how are you doing?\n",
      "Sentence  2 :  Such a wonderful day it is.\n",
      "Sentence  3 :  How do you do folks?\n",
      "Sentence  4 :  What time is it right now?\n",
      "Sentence  5 :  When is your birthday?\n",
      "Sentence  6 :  When do you usually return from work?\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define x and y coordinates for the grid\n",
    "# each axis represents sentences in the list\n",
    "# the color in grid formed by the intersection of i-th and j-th sections of the axis\n",
    "# gives the similarity between the corresponding i-th and j-th sentences from the list\n",
    "X = np.arange(0,len(sentences))\n",
    "Y = np.arange(0,len(sentences))\n",
    "\n",
    "# create all combinations of sentence indices using meshgrid fucntion\n",
    "X,Y = np.meshgrid(X,Y)\n",
    "\n",
    "# flatten them for iteration\n",
    "X_t = X.reshape(-1)\n",
    "Y_t = Y.reshape(-1)\n",
    "\n",
    "# initilize list for computing similarity value\n",
    "Z = []\n",
    "\n",
    "\n",
    "for i in range(len(X_t)): # for every pair of sentences compute their similarity\n",
    "    Z.append(cosine_similarity(vectors[X_t[i]],vectors[Y_t[i]]))\n",
    "\n",
    "# reshape the Z array into 2-D grid for visualization\n",
    "Z = np.array(Z).reshape(len(sentences),len(sentences))\n",
    "\n",
    "# show the gird and colorbar\n",
    "plt.imshow(Z)\n",
    "plt.colorbar()\n",
    "\n",
    "# print the list of sentences for ease of comparision\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(\"Sentence \", i ,\": \", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57ed788194e52380de63de149edab0c9",
     "grade": false,
     "grade_id": "cell-060d24f241b1d2ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In the above grid, we see that the colors in the main diagonal is `yellow`, representing maximum similarity, which is as expected because a sentence is indeed similar to itself. Sentence 0 and 2 have `green` color indicating high similarity which is as expected. This form of similarity works well for many use cases, but fails when the vocabulary grows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "8UrntiTSA-2i",
    "nbgrader": {
     "checksum": "fdf298a303cd96728adda019e5bf25d5",
     "grade": false,
     "grade_id": "cell-cbdf577921181636",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Visualize word2vec embedding vectors\n",
    "In the above example, we simply tokenized sentences by considering every word as a distinct token and vectorized the senteces with respect to those tokens. This process though useful, is limited due to its inability to understand the similarity between words through their meaning. Also, such form of vectorization gives sparse representation for sentences, due to which other forms of similarity measures such as (or L2) distance, Manhattan (or L1) distance do not give meaning and addition of vectors also do not make much sense.\n",
    "\n",
    "While performing natural language processing (NLP) with deep learning, words are vectorized by learning dense embeddings. These embeddings are vector representation of words and loosely follow the laws of vector spaces. Addition with such vectors produce sensible results, the examples of which you can visualize by running the cells below.\n",
    "\n",
    "In this example, we take limited words from a pretrained model of word2vec stored as a dictionary in the \"*embeddings.pickle*\" file. Here, we treat the word2vec as a black box model and only manipulate vectors and measure the notion of similarity. The embeddings were obtained from the following source and has the characteristics mentioned below:\n",
    "\n",
    "**Source:**: https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "Number of Instances: 1 million\n",
    "\n",
    "Number of Dimensions: 300\n",
    "\n",
    "---\n",
    "\n",
    "- The dataset consists of 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3bd06b3dfb75cb90c8a25903040c70a2",
     "grade": false,
     "grade_id": "cell-67aa4d4ecd0ec0e4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'embeddings.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c6df139283a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the pickle file \"embeddings.pickle\" into the word_vectors dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'embeddings.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'embeddings.pickle'"
     ]
    }
   ],
   "source": [
    "# load the pickle file \"embeddings.pickle\" into the word_vectors dictionary\n",
    "with open('embeddings.pickle', 'rb') as handle:\n",
    "    word_vectors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "GExBUvHOXcb6",
    "nbgrader": {
     "checksum": "a84f5f0570de77b164e2dbefa7c3c0e5",
     "grade": false,
     "grade_id": "cell-51b49e417cf63a4b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# given a vector find the most similar word from the dictionary of word vectors\n",
    "def most_similar_vector(vector, word_vectors):\n",
    "    mx = 0 # initialize maximum value\n",
    "    argmx = None # initialize the word to None\n",
    "    for key,v in word_vectors.items():\n",
    "        sim = cosine_similarity(v,vector)\n",
    "        if(sim>mx):\n",
    "            mx = sim\n",
    "            argmx = key\n",
    "    return argmx, mx # return word, similarity value\n",
    "\n",
    "# given a word already in the dict find the most similar word other than itself from the dictionary of word vectors\n",
    "def most_similar_word(word, word_vectors):\n",
    "    mx = 0\n",
    "    argmx = None\n",
    "    for key,v in word_vectors.items():\n",
    "        if(word!=key):\n",
    "            sim = cosine_similarity(v,word_vectors[word])\n",
    "        if(sim>mx):\n",
    "            mx = sim\n",
    "            argmx = key\n",
    "    return argmx, mx  # return word, similarity value\n",
    "\n",
    "# print the words for which we have embeddings\n",
    "print(\"Total number of words: \",len(word_vectors.items()))\n",
    "for key, value in word_vectors.items():\n",
    "    print(key, end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "6UhiWw5KJwkx",
    "nbgrader": {
     "checksum": "54efb23bdcab6256a3ea85dc1e2f5fe3",
     "grade": false,
     "grade_id": "cell-9134e11d12f9b83e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In the example below, using the word2vec embeddings, we can see that by removing some scale of `man` from `king` and adding some scale of `woman` to it makes the vector most similar to `queen`. This operation makes sense intuitively and using deep learning we are able to capture such meanings and relations between words in the form of vectors. We also see that the word `mountain` is most similar to `hill` from the list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "X4wg7oiEQQ8f",
    "nbgrader": {
     "checksum": "fcdb4726c73c05346ea5f557763818c2",
     "grade": false,
     "grade_id": "cell-4caed24057d1dbad",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "53db2236-a954-4651-807f-b7a710617de1"
   },
   "outputs": [],
   "source": [
    "print(most_similar_vector(word_vectors[\"king\"]-2*word_vectors[\"man\"]+1.75*word_vectors[\"woman\"],word_vectors))\n",
    "print(most_similar_word(\"hill\",word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "Assignment: Scalars, Vectors and their operations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
